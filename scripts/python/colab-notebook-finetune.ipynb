{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/levelup-apps/hn-enhancer/blob/main/scripts/python/colab-notebook-finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finetune AI summarization of HN comments - HN Companion\n",
        "This notebook contains the code to finetune the task of summarizing HN comments.\n",
        "\n",
        "In the HNCompanion browser extension, we use LLMs like OpenAI, Claude or any model to generate summaries with a big system prompt. The results were varied based on the models, but by far, GPT-4, o1-mini, Claude and DeepSeek have given the best results in terms of meaningful summary and proper backlinks to link from summary to the relevant threads. But these models are expensive, especially to run continuosuly (multiple times a day) to summarize HN posts. We believe that we can get finetune a less expensive open model (DeepSeek distilled, Llama etc.) with good data generated by expensive frontier models, we will ge better results from the less capable model.\n",
        "\n",
        "**Our hypothesis** - Finetuning a generic lower-end models on AI summaries generated by more powerful and expensive models will improve the quality of output generated by the lower-end models.\n",
        "\n",
        "The concept is similar to **knowledge distillation**, where a smaller model learns from a more powerful one. We have access to high-quality training data from top models that demonstrate the desired behavior. We use that data to teach a specific, bounded task (HN post summarization) rather than general capabilities.\n",
        "\n",
        "If our hypothesis can be proven, we have a great solution that balances cost and performance. Then we can generate summaries in advance (on top 30 / 60 posts), cache them and serve them. This will grealty inprove the user experience and performance of 'summarize' workflows.\n",
        "\n",
        "**Potential benefits** - Cost reduction for inference, faster inference times, ability to run summaries proactively rather than on-demand, more consistent output format and quality.\n",
        "\n",
        "### General Approach\n",
        "\n",
        "**Building dataset**\n",
        "* ensure diversity in HN posts (technical, business, general discussion)\n",
        "* include different post lengths and styles\n",
        "* capture edge cases in the dataset\n",
        "\n",
        "**Select base model**\n",
        "* select a base model that is capable enough to learn the task\n",
        "* base model should handle technical vocabulary well\n",
        "* must be able to process longer inputs effectively\n",
        "\n",
        "**Train & Evaluate**\n",
        "* start with a small dataset (500 - 1000 examples)\n",
        "* try with different prompt formats, validate with a test set\n",
        "* evaluate accuracy of backlinks, consistency\n",
        "* preservation of technical vocab and HN vibe\n",
        "* do human eval of sample set if needed\n",
        "\n",
        "This notebook is a customized version of Unsloth example to finetune any model to have a chat conversation and export it in GGUF format to run on Ollama.\n"
      ],
      "metadata": {
        "id": "_V9nyWBicX0g"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IqM-T1RTzY6C"
      },
      "source": [
        "## How to run this notebook\n",
        "Select each cell and run or select Runtime and Run All.\n",
        "\n",
        "Finetuning happens in a few steps:\n",
        "\n",
        "1. [Select the base model](#Select)\n",
        "2. [Load the training dataset from HuggingFace](#Data)\n",
        "3. [Train the model](#Train)\n",
        "4. [Run the model through inference](#Inference)\n",
        "5. [Export the finetuned model to Ollama](#Ollama)\n",
        "\n",
        "### References\n",
        "* Install Unsloth on your own computer - [Unsloth Github page](https://github.com/unslothai/unsloth#installation-instructions---conda)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare the environment"
      ],
      "metadata": {
        "id": "oKLc_ZhkFQ7S"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2eSvM9zX_2d3"
      },
      "outputs": [],
      "source": [
        "#@title Install dependencies\n",
        "%%capture\n",
        "!pip install unsloth\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<a name=\"Select\"></a>\n",
        "### Select the model\n",
        "We will finetune first with Lllama-3 8-bit and then with DeepSeek distill model."
      ],
      "metadata": {
        "id": "TzSd1h2x8-cw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "985f0dd9-077e-4a20-c1c2-b727c89d771f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==((====))==  Unsloth 2025.2.9: Fast Llama patching. Transformers: 4.48.2.\n",
            "   \\\\   /|    GPU: Tesla T4. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
            " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "\n",
            "Initialized base model - name: unsloth/llama-3.2-3b-instruct-bnb-4bit, max_seq_length: 4096\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 4096 # Context length.\n",
        "  # TODO: Increase to 8192 or higher to match the context window length of final model.\n",
        "  # Refer https://unsloth.ai/blog/long-context. RTX 4090 can take upto 56K context length (input token)\n",
        "\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Set to False for higher (1-2%) accuracy\n",
        "\n",
        "# 4bit pre quantized models we support for 4x faster downloading + no OOMs.\n",
        "fourbit_models = [\n",
        "    \"unsloth/mistral-7b-v0.3-bnb-4bit\",      # New Mistral v3 2x faster!\n",
        "    \"unsloth/mistral-7b-instruct-v0.3-bnb-4bit\",\n",
        "    \"unsloth/llama-3-8b-bnb-4bit\",           # Llama-3 15 trillion tokens model 2x faster!\n",
        "    \"unsloth/llama-3-8b-Instruct-bnb-4bit\",\n",
        "    \"unsloth/llama-3-70b-bnb-4bit\",\n",
        "    \"unsloth/Phi-3-mini-4k-instruct\",        # Phi-3 2x faster!\n",
        "    \"unsloth/Phi-3-medium-4k-instruct\",\n",
        "    \"unsloth/mistral-7b-bnb-4bit\",\n",
        "    \"unsloth/gemma-7b-bnb-4bit\",             # Gemma 2.2x faster!\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "# base_model_name = \"unsloth/llama-3-8b-bnb-4bit\"\n",
        "# base_model_name = \"unsloth/DeepSeek-R1-Distill-Llama-8B-unsloth-bnb-4bit\"\n",
        "# base_model_name = \"unsloth/Llama-3.3-70B-Instruct-bnb-4bit\"\n",
        "base_model_name = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"\n",
        "\n",
        "base_model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = base_model_name,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit\n",
        "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
        ")\n",
        "\n",
        "print(f\"\\nInitialized base model - name: {base_model.name_or_path}, max_seq_length: {max_seq_length}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXd9bTZd1aaL"
      },
      "source": [
        "We now add LoRA adapters so we only need to update 1 to 10% of all parameters!\n",
        "\n",
        "LoRA = **Low Rank Adaptation**, the highly efficient finetuning technique where you make small targeted adjustments to the base model in order to customize it to do specific tasks.\n",
        "\n",
        "You adjust only a small set of parameters (low-rank matrices) which are applied on top of the base model to get results specific to the new context. This matrix adjusts for the weights of the original model so that the results match the desired outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "outputs": [],
      "source": [
        "lora_model = FastLanguageModel.get_peft_model(\n",
        "    base_model,\n",
        "    r = 16, # Rank of the finetuning process. We can choose any number > 0. Suggested 8, 16, 32, 64, 128\n",
        "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_alpha = 16, # Scaling factor for finetuning. A large number may cause overfitting.\n",
        "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
        "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
        "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context (uses 30% less VRAM, fits 2x larger batch sizes!)\n",
        "    random_state = 3407, # number to determine deterministic runs\n",
        "    use_rslora = False,  # rslora: rank stabilized LoRA\n",
        "    loftq_config = None, # LoftQ - Advanced feature, can improve accuracy somewhat, but can make memory usage explode at the start.\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data preparation\n",
        "We now use the HN comments dataset from HuggingFace, which is the collection of comments/summapry pairs for many HN posts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvOPfPnet76H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "660fa19a-61e2-4c43-dd66-50561c1fd35b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train dataset size: 6\n",
            "Validation dataset size: 2\n",
            "Test dataset size: 1\n",
            "\n",
            "Filtered out 0 items from training dataset\n",
            "Original dataset size: 6\n",
            "Filtered dataset size: 6\n",
            "\n",
            "HN dataset: Dataset({\n",
            "    features: ['post_id', 'input_comment', 'output_summary'],\n",
            "    num_rows: 6\n",
            "})\n",
            "selected post_ids: ['42803774', '42931109', '42901616', '42684257', '42889786', '42681762']\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import json\n",
        "from google.colab import userdata\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "\n",
        "# hf_dataset_id = \"annjose/hn-comments\"\n",
        "hf_dataset_id = \"annjose/hn-comments-small\"\n",
        "\n",
        "train_dataset = load_dataset(hf_dataset_id, split = \"train\", token = hf_token)\n",
        "print(f\"Train dataset size: {len(train_dataset)}\")\n",
        "\n",
        "val_dataset = load_dataset(hf_dataset_id, split = \"validation\", token = hf_token)\n",
        "print(f\"Validation dataset size: {len(val_dataset)}\")\n",
        "\n",
        "test_dataset = load_dataset(hf_dataset_id, split = \"test\", token = hf_token)\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "\n",
        "# Define the post_ids to exclude (these two posts have > 120K character length)\n",
        "# This is the list of post ids with the length of their final text (prompt + comments) and token length\n",
        "#   Row 1: post_id: 42608155. Text length: 146182.\tToken count: 38724\n",
        "#   Row 2: post_id: 42607623. Text length: 124933.\tToken count: 29953\n",
        "#   Row 3: post_id: 42611536. Text length: 121611.\tToken count: 34908\n",
        "#   Row 4: post_id: 42609819. Text length:  12748.\tToken count:  3168\n",
        "#   Row 5: post_id: 42607794. Text length:  17506.\tToken count:  4684\n",
        "#   Row 6: post_id: 42609595. Text length:  31676.\tToken count:  7694\n",
        "\n",
        "# this exclusion works - training runs ok, but inference is not meaningful\n",
        "# exclude_ids = ['42608155', '42607623', '42611536',]\n",
        "\n",
        "# this exclusion did not work on L4 TPU with max-seq_ 2048\n",
        "# exclude_ids = ['42608155', '42607623']\n",
        "\n",
        "# exclude_ids = ['42608155'] # this didn't work even with max_context_length 36K\n",
        "\n",
        "# in the smaller dataset, exclude two posts with 3000+ token length\n",
        "exclude_ids = [] # ['42889786', '42901616']\n",
        "\n",
        "# Filter the dataset to keep only rows where post_id is not in exclude_ids\n",
        "filtered_dataset = train_dataset.filter(lambda x: x['post_id'] not in exclude_ids)\n",
        "\n",
        "print(f\"\\nFiltered out {len(exclude_ids)} items from training dataset\")\n",
        "print(f\"Original dataset size: {len(train_dataset)}\")\n",
        "print(f\"Filtered dataset size: {len(filtered_dataset)}\")\n",
        "\n",
        "# dataset_hn = train_dataset\n",
        "dataset_hn = filtered_dataset\n",
        "print(f\"\\nHN dataset: {dataset_hn}\")\n",
        "print(f\"selected post_ids: {filtered_dataset['post_id']}\")\n",
        "\n",
        "# print(\"\\nFirst data row...\")\n",
        "# print(json.dumps(dataset_hn[0], indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Formatting - LLama 3.3 Instruct\n"
      ],
      "metadata": {
        "id": "cBW20gahMe9B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# RUN THIS - Format the dataset to match the base model's template and validate it.\n",
        "\n",
        "llama_template = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "{SYSTEM}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "{INPUT}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "{OUTPUT}<|eot_id|>\"\"\"\n",
        "\n",
        "prompt_template = llama_template\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token # Must add EOS_TOKEN\n",
        "\n",
        "system_prompt = \"\"\"You are an AI assistant specialized in analyzing and summarizing Hacker News discussions.\n",
        "A discussion consists of threaded comments where each comment can have child comments (replies) nested underneath it,\n",
        "forming interconnected conversation branches. Your task is to provide concise, meaningful summaries that capture the\n",
        "essence of the discussion while prioritizing engaging and high quality content.\"\"\"\n",
        "\n",
        "user_prompt_prefix = \"This is your input:\\n The title of the post and comments are separated by dashed lines.\"\n",
        "\n",
        "def format_prompts_func(examples):\n",
        "\n",
        "    try:\n",
        "        post_ids = examples[\"post_id\"]\n",
        "        input_comments = examples[\"input_comment\"]\n",
        "        summaries = examples[\"output_summary\"]\n",
        "\n",
        "        formatted_texts = []\n",
        "        for post_id, comment, summary in zip(post_ids, input_comments, summaries):\n",
        "            # Validate inputs\n",
        "            if not all([comment, summary]):\n",
        "                continue\n",
        "\n",
        "            # Create the prompt by combining system prompt and user prompt (comment)\n",
        "            user_prompt = f\"{user_prompt_prefix}\\n{comment}\"\n",
        "\n",
        "            # Apply the data (prompt and summary) to the template format\n",
        "            formatted_text = prompt_template.format(\n",
        "                SYSTEM=system_prompt,\n",
        "                INPUT=user_prompt,\n",
        "                OUTPUT=summary\n",
        "            ) + EOS_TOKEN\n",
        "\n",
        "            formatted_texts.append(formatted_text)\n",
        "\n",
        "        return {\"text\": formatted_texts}\n",
        "    except Exception as e:\n",
        "        print(f\"Error formatting prompts: {e}\")\n",
        "        raise\n",
        "\n",
        "print(f\"BEFORE formatting: Column Names: {dataset_hn.column_names}. Row count: {dataset_hn.num_rows}\")\n",
        "\n",
        "# Format the text in the dataset according to the base model's prompt template\n",
        "dataset = dataset_hn.map(format_prompts_func, batched = True,)\n",
        "\n",
        "print(f\"AFTER formatting: Column Names: {dataset.column_names}. Row count: {dataset.num_rows}\")\n",
        "\n",
        "import json\n",
        "print(\"\\ndataset[0]...\")\n",
        "print(json.dumps(dataset[0], indent=2))\n",
        "\n",
        "# print the length of 'text' property in the dataset\n",
        "print(\"\\nLength of 'text' property in the datasets:\")\n",
        "total_token_count = 0\n",
        "longest_token_length = 0 # longest token\n",
        "\n",
        "for idx, row in enumerate(dataset):\n",
        "  text = row['text']\n",
        "  tokens = tokenizer.encode(text)\n",
        "\n",
        "  token_count = len(tokens)\n",
        "  total_token_count += token_count\n",
        "  longest_token_length = max(longest_token_length, token_count)\n",
        "\n",
        "  print(f\"Row {idx + 1}: post_id: {row['post_id']}. Text length: {len(text)}.\\tToken count: {token_count}\")\n",
        "\n",
        "\n",
        "print(f\"\\nTotal tokens in dataset: {total_token_count}. Longest token length in dataset: {longest_token_length}. Model's max token length: {max_seq_length}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eWyyXEjgMbuY",
        "outputId": "ebcca7b5-1a90-4378-de39-fc5b8011eefa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BEFORE formatting: Column Names: ['post_id', 'input_comment', 'output_summary']. Row count: 6\n",
            "AFTER formatting: Column Names: ['post_id', 'input_comment', 'output_summary', 'text']. Row count: 6\n",
            "\n",
            "dataset[0]...\n",
            "{\n",
            "  \"post_id\": \"42803774\",\n",
            "  \"input_comment\": \"---- Post Title: \\nAn overview of gradient descent optimization algorithms (2016)\\n----- Comments: \\n[1] (score: 1000) <replies: 3> janalsncm: Article is from 2016. It only mentions AdamW at the very end in passing. These days I rarely see much besides AdamW in production.Messing with optimizers is one of the ways to enter hyperparameter hell: it\\u2019s like legacy code but on steroids because changing it only breaks your training code stochastically. Much better to stop worrying and love AdamW.\\n[1.1] (score: 961) <replies: 0> nkurz: The mention of AdamW is brief, but in his defense he includes a link that gives a gloss of it: \\\"An updated overview of recent gradient descent algorithms\\\" [].\\n[1.2] (score: 923) <replies: 1> pizza: Luckily we have Shampoo, SOAP, Modula, Schedule-free variants, and many more these days being researched! I am very very excited by the heavyball library in particular\\n[1.2.1] (score: 884) <replies: 1> 3abiton: Been out of the loop for while, anything exciting?\\n[1.2.1.1] (score: 846) <replies: 0> pizza: Read the Modular Norms in Deep Learning paper, and follow the author of the heavyball library on twitter with notifications enabled\\n[1.3] (score: 807) <replies: 1> ImageXav: Something that stuck out to me in the updated blog [0] is that Demon Adam performed much better than even AdamW, with very interesting learning curves. I'm wondering now why it didn't become the standard. Anyone here have insights into this?[0] \\n[1.3.1] (score: 769) <replies: 0> gzer0: Demon Adam didn\\u2019t become standard largely for the same reason many \\u201cbetter\\u201d optimizers never see wide adoption: it\\u2019s a newer tweak, not clearly superior on every problem, is less familiar to most engineers, and isn\\u2019t always bundled in major frameworks. By contrast, AdamW is now the \\u201csafe default\\u201d that nearly everyone supports and knows how to tune, so teams stick with it unless they have a strong reason not to.Edit: Demon involves decaying the momentum parameter over time, which introduces a new schedule or formula for how momentum should be reduced during training. That can feel like additional complexity or a potential hyperparameter rabbit hole. Teams trying to ship products quickly often avoid adding new hyperparameters unless the gains are decisive.\\n[2] (score: 730) <replies: 2> sega_sai: Interesting, but it does not seem to be an overview of gradient optimisers, but rather gradient optimisers in ML, as I see no mentions of BFGS and the likes.\\n[2.1] (score: 692) <replies: 5> VHRanger: I'm also curious about gradient-less algorithmsFor non deep learning applications, Nelder-Mead saved my butt a fees times\\n[2.1.1] (score: 653) <replies: 0> analog31: It's with the utmost humility that I confess to falling back on \\\"just use Nelder-Mead\\\" in *scipy.optimize* when something is ill behaved. I consider it to be a sign that I'm doing something wrong, but I certainly respect its use.\\n[2.1.2] (score: 615) <replies: 0> imurray: Nelder\\u2013Mead has often not worked well for me in moderate to high dimensions. I'd recommend trying Powell's method if you want to quickly converge to a local optimum. If you're using scipy's wrappers, it's easy to swap between the two:For nastier optimization problems there are lots of other options, including evolutionary algorithms and Bayesian optimization:\\n[2.1.3] (score: 576) <replies: 0> kernc: SAMBO does a good job of finding the global optimum in a black-box manner even compared to Nelder-Mead, according to its own benchmark ...\\n[2.1.4] (score: 538) <replies: 0> amelius: ChatGPT also advised me to use NM a couple of times, which was neat.\\n[2.1.5] (score: 500) <replies: 0> woadwarrior01: Look into zeroth-order optimizers and CMA-ES.\\n[2.2] (score: 461) <replies: 1> mike-the-mikado: I think the big difference is dimensionality.  If the dimensionality is low, then taking account of the 2nd derivatives becomes practical and worthwhile.\\n[2.2.1] (score: 423) <replies: 1> juliangoldsmith: What is it that makes higher order derivatives less useful at high dimensionality?  Is it related to the Curse of Dimensionality, or maybe something like exploding gradients at higher orders?\\n[2.2.1.1] (score: 384) <replies: 0> mike-the-mikado: In n dimensions, the first derivative is an n-element vector.  The second derivative is an n x n (symmetric) matrix.  As n grows, the computation required to estimate the matrix increases (as at least n^2) and computation needed to use it increases (possibly faster).In practice, clever optimisation algorithms that use the 2nd derivative won't actually form this matrix.\\n[3] (score: 346) <replies: 0> ipunchghosts: Example of thr bitter lesson. None of these nuanced matter 8 years later where everyone uses sgd or adamw.\\n[4] (score: 307) <replies: 1> sk11001: It's a great summary for ML interview prep.\\n[4.1] (score: 269) <replies: 1> janalsncm: I disagree, it is old and most of those algorithms aren\\u2019t used anymore.\\n[4.1.1] (score: 230) <replies: 3> sk11001: That\\u2019s how interviews go though, it\\u2019s not like I\\u2019ve ever had to use Bayes rule at work but for a few years everyone loved asking about it in screening rounds.\\n[4.1.1.1] (score: 192) <replies: 0> mike-the-mikado: In my experience a lot of people \\\"know\\\" maths, but fail to recognise the opportunities to use it.  Some of my colleagues were pleased when I showed them that their ad hoc algorithm was equivalent to an application of Bayes' rule.  It gave them insights into the meaning of constants that had formerly been chosen by trial and error.\\n[4.1.1.2] (score: 153) <replies: 1> janalsncm: Everyone\\u2019s experience is different but I\\u2019ve been in dozens of MLE interviews (some of which I passed!) and have never once been asked to explain the internals of an optimizer. The interviews were all post 2020, though.Unless someone had a very good reason I would consider it weird to use anything other than AdamW. The compute you could save on a slightly better optimizer pale in comparison to the time you will spend debugging an opaque training bug.\\n[4.1.1.2.1] (score: 115) <replies: 0> yobbo: For example, if it is meaningful to use large batch sizes, the gradient variance will be lower and adam could be equivalent to just momentum.As a model is trained, the gradient variance typically falls.Those optimizers all work to reduce the variance of the updates in various ways.\\n[4.1.1.3] (score: 76) <replies: 1> esafak: I'd still expect an MLE to know it though.\\n[4.1.1.3.1] (score: 38) <replies: 0> janalsncm: Why would you? Implementing optimizers isn\\u2019t something that MLEs do. Even the Deepseek team just uses AdamW.An MLE should be able to look up and understand the differences between optimizers but memorizing that information is extremely low priority compared with other information they might be asked.\\n\",\n",
            "  \"output_summary\": \"# Overview\\nThe discussion centers around a 2016 article providing an overview of gradient descent optimization algorithms for machine learning. Participants discuss the relevance of the article in the context of more recent advancements, with a focus on the prominence of AdamW. The conversation explores the advantages and disadvantages of various optimization algorithms, their practical application, and the factors that influence their adoption in production environments.\\n\\n# Main Themes & Key Insights\\n\\n*   **Relevance of the Article**:  The age of the article (2016) is a central point, as newer algorithms like AdamW are now widely used. ([1], score: 1000, replies: 3) Participants emphasize the importance of staying up-to-date with the latest optimization techniques.\\n*   **AdamW's Dominance**: AdamW is presented as a current \\\"safe default\\\" optimizer, favored for its ease of use and broad support in machine learning frameworks. ([1], score: 1000, replies: 3) The article explores the trade-offs between using AdamW and experimenting with other optimizers.\\n*   **Optimizer Choice and Hyperparameter Tuning**:  The discussion highlights the challenges of hyperparameter tuning and the potential for experimenting with optimizers to create \\\"hyperparameter hell.\\\" ([1], score: 1000, replies: 3)\\n*   **Alternative and Gradient-less Algorithms**: The conversation touches on more recent advancements in optimization algorithms, including Shampoo, SOAP, and heavyball, as well as gradient-less algorithms like Nelder-Mead. ([1.2], score: 923, replies: 1)\\n*   **Relevance to Interviews and Practical Use**: The discussion delves into the role of optimization algorithms in machine learning interviews. ([4.1], score: 269, replies: 1) Participants note that while theoretical knowledge is valuable, practical considerations like framework support and ease of use often drive algorithm choices in production.\\n\\n# Relevance of the Article\\nThis theme discusses the age of the article and the implications for the current state of optimization algorithms.\\n*   The article is from 2016. ([1], score: 1000)\\n*   Users state that the algorithm discussed in the article are rarely used now. ([4.1], score: 269)\\n\\n# AdamW's Dominance\\nThis theme highlights AdamW's prominence as a widely-used optimizer.\\n*   AdamW is used, and is considered a \\\"safe default\\\". ([1], score: 1000)\\n\\n    *   \\\"These days I rarely see much besides AdamW in production.Messing with optimizers is one of the ways to enter hyperparameter hell: it\\u2019s like legacy code but on steroids because changing it only breaks your training code stochastically. Much better to stop worrying and love AdamW.\\\" ([1], score: 1000)\\n*   Teams tend to stick with AdamW due to its ease of use and wide support. ([1.3.1], score: 769)\\n\\n# Optimizer Choice and Hyperparameter Tuning\\nThis theme touches on the complexities of selecting and tuning optimization algorithms.\\n*   Messing with optimizers can lead to \\\"hyperparameter hell\\\". ([1], score: 1000)\\n*   Demon Adam did not become standard, for similar reasons. ([1.3.1], score: 769)\\n\\n    *   \\\"Demon Adam didn\\u2019t become standard largely for the same reason many \\u201cbetter\\u201d optimizers never see wide adoption: it\\u2019s a newer tweak, not clearly superior on every problem, is less familiar to most engineers, and isn\\u2019t always bundled in major frameworks.\\\" ([1.3.1], score: 769)\\n*   The discussion touches on how complex the optimization algorithm can get. ([2.2.1], score: 423)\\n\\n    *   \\\"In n dimensions, the first derivative is an n-element vector. The second derivative is an n x n (symmetric) matrix. As n grows, the computation required to estimate the matrix increases (as at least n^2) and computation needed to use it increases (possibly faster).\\\" ([2.2.1], score: 423)\\n\\n# Alternative and Gradient-less Algorithms\\nThis theme covers alternative options for optimization.\\n*   Other optimizers are mentioned. ([1.2], score: 923)\\n    *   \\\"Luckily we have Shampoo, SOAP, Modula, Schedule-free variants, and many more these days being researched!\\\" ([1.2], score: 923)\\n*   Gradient-less algorithms are also discussed, and one user likes Nelder-Mead for non deep learning applications. ([2.1], score: 692)\\n\\n# Relevance to Interviews and Practical Use\\nThis theme looks at the information related to machine learning interviews.\\n*   The article is a good summary. ([4], score: 307)\\n*   Users state that the algorithms mentioned may be needed in interviews. ([4.1.1], score: 230)\\n\\n\",\n",
            "  \"text\": \"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are an AI assistant specialized in analyzing and summarizing Hacker News discussions.\\nA discussion consists of threaded comments where each comment can have child comments (replies) nested underneath it,\\nforming interconnected conversation branches. Your task is to provide concise, meaningful summaries that capture the\\nessence of the discussion while prioritizing engaging and high quality content.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nThis is your input:\\n The title of the post and comments are separated by dashed lines.\\n---- Post Title: \\nAn overview of gradient descent optimization algorithms (2016)\\n----- Comments: \\n[1] (score: 1000) <replies: 3> janalsncm: Article is from 2016. It only mentions AdamW at the very end in passing. These days I rarely see much besides AdamW in production.Messing with optimizers is one of the ways to enter hyperparameter hell: it\\u2019s like legacy code but on steroids because changing it only breaks your training code stochastically. Much better to stop worrying and love AdamW.\\n[1.1] (score: 961) <replies: 0> nkurz: The mention of AdamW is brief, but in his defense he includes a link that gives a gloss of it: \\\"An updated overview of recent gradient descent algorithms\\\" [].\\n[1.2] (score: 923) <replies: 1> pizza: Luckily we have Shampoo, SOAP, Modula, Schedule-free variants, and many more these days being researched! I am very very excited by the heavyball library in particular\\n[1.2.1] (score: 884) <replies: 1> 3abiton: Been out of the loop for while, anything exciting?\\n[1.2.1.1] (score: 846) <replies: 0> pizza: Read the Modular Norms in Deep Learning paper, and follow the author of the heavyball library on twitter with notifications enabled\\n[1.3] (score: 807) <replies: 1> ImageXav: Something that stuck out to me in the updated blog [0] is that Demon Adam performed much better than even AdamW, with very interesting learning curves. I'm wondering now why it didn't become the standard. Anyone here have insights into this?[0] \\n[1.3.1] (score: 769) <replies: 0> gzer0: Demon Adam didn\\u2019t become standard largely for the same reason many \\u201cbetter\\u201d optimizers never see wide adoption: it\\u2019s a newer tweak, not clearly superior on every problem, is less familiar to most engineers, and isn\\u2019t always bundled in major frameworks. By contrast, AdamW is now the \\u201csafe default\\u201d that nearly everyone supports and knows how to tune, so teams stick with it unless they have a strong reason not to.Edit: Demon involves decaying the momentum parameter over time, which introduces a new schedule or formula for how momentum should be reduced during training. That can feel like additional complexity or a potential hyperparameter rabbit hole. Teams trying to ship products quickly often avoid adding new hyperparameters unless the gains are decisive.\\n[2] (score: 730) <replies: 2> sega_sai: Interesting, but it does not seem to be an overview of gradient optimisers, but rather gradient optimisers in ML, as I see no mentions of BFGS and the likes.\\n[2.1] (score: 692) <replies: 5> VHRanger: I'm also curious about gradient-less algorithmsFor non deep learning applications, Nelder-Mead saved my butt a fees times\\n[2.1.1] (score: 653) <replies: 0> analog31: It's with the utmost humility that I confess to falling back on \\\"just use Nelder-Mead\\\" in *scipy.optimize* when something is ill behaved. I consider it to be a sign that I'm doing something wrong, but I certainly respect its use.\\n[2.1.2] (score: 615) <replies: 0> imurray: Nelder\\u2013Mead has often not worked well for me in moderate to high dimensions. I'd recommend trying Powell's method if you want to quickly converge to a local optimum. If you're using scipy's wrappers, it's easy to swap between the two:For nastier optimization problems there are lots of other options, including evolutionary algorithms and Bayesian optimization:\\n[2.1.3] (score: 576) <replies: 0> kernc: SAMBO does a good job of finding the global optimum in a black-box manner even compared to Nelder-Mead, according to its own benchmark ...\\n[2.1.4] (score: 538) <replies: 0> amelius: ChatGPT also advised me to use NM a couple of times, which was neat.\\n[2.1.5] (score: 500) <replies: 0> woadwarrior01: Look into zeroth-order optimizers and CMA-ES.\\n[2.2] (score: 461) <replies: 1> mike-the-mikado: I think the big difference is dimensionality.  If the dimensionality is low, then taking account of the 2nd derivatives becomes practical and worthwhile.\\n[2.2.1] (score: 423) <replies: 1> juliangoldsmith: What is it that makes higher order derivatives less useful at high dimensionality?  Is it related to the Curse of Dimensionality, or maybe something like exploding gradients at higher orders?\\n[2.2.1.1] (score: 384) <replies: 0> mike-the-mikado: In n dimensions, the first derivative is an n-element vector.  The second derivative is an n x n (symmetric) matrix.  As n grows, the computation required to estimate the matrix increases (as at least n^2) and computation needed to use it increases (possibly faster).In practice, clever optimisation algorithms that use the 2nd derivative won't actually form this matrix.\\n[3] (score: 346) <replies: 0> ipunchghosts: Example of thr bitter lesson. None of these nuanced matter 8 years later where everyone uses sgd or adamw.\\n[4] (score: 307) <replies: 1> sk11001: It's a great summary for ML interview prep.\\n[4.1] (score: 269) <replies: 1> janalsncm: I disagree, it is old and most of those algorithms aren\\u2019t used anymore.\\n[4.1.1] (score: 230) <replies: 3> sk11001: That\\u2019s how interviews go though, it\\u2019s not like I\\u2019ve ever had to use Bayes rule at work but for a few years everyone loved asking about it in screening rounds.\\n[4.1.1.1] (score: 192) <replies: 0> mike-the-mikado: In my experience a lot of people \\\"know\\\" maths, but fail to recognise the opportunities to use it.  Some of my colleagues were pleased when I showed them that their ad hoc algorithm was equivalent to an application of Bayes' rule.  It gave them insights into the meaning of constants that had formerly been chosen by trial and error.\\n[4.1.1.2] (score: 153) <replies: 1> janalsncm: Everyone\\u2019s experience is different but I\\u2019ve been in dozens of MLE interviews (some of which I passed!) and have never once been asked to explain the internals of an optimizer. The interviews were all post 2020, though.Unless someone had a very good reason I would consider it weird to use anything other than AdamW. The compute you could save on a slightly better optimizer pale in comparison to the time you will spend debugging an opaque training bug.\\n[4.1.1.2.1] (score: 115) <replies: 0> yobbo: For example, if it is meaningful to use large batch sizes, the gradient variance will be lower and adam could be equivalent to just momentum.As a model is trained, the gradient variance typically falls.Those optimizers all work to reduce the variance of the updates in various ways.\\n[4.1.1.3] (score: 76) <replies: 1> esafak: I'd still expect an MLE to know it though.\\n[4.1.1.3.1] (score: 38) <replies: 0> janalsncm: Why would you? Implementing optimizers isn\\u2019t something that MLEs do. Even the Deepseek team just uses AdamW.An MLE should be able to look up and understand the differences between optimizers but memorizing that information is extremely low priority compared with other information they might be asked.\\n<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n# Overview\\nThe discussion centers around a 2016 article providing an overview of gradient descent optimization algorithms for machine learning. Participants discuss the relevance of the article in the context of more recent advancements, with a focus on the prominence of AdamW. The conversation explores the advantages and disadvantages of various optimization algorithms, their practical application, and the factors that influence their adoption in production environments.\\n\\n# Main Themes & Key Insights\\n\\n*   **Relevance of the Article**:  The age of the article (2016) is a central point, as newer algorithms like AdamW are now widely used. ([1], score: 1000, replies: 3) Participants emphasize the importance of staying up-to-date with the latest optimization techniques.\\n*   **AdamW's Dominance**: AdamW is presented as a current \\\"safe default\\\" optimizer, favored for its ease of use and broad support in machine learning frameworks. ([1], score: 1000, replies: 3) The article explores the trade-offs between using AdamW and experimenting with other optimizers.\\n*   **Optimizer Choice and Hyperparameter Tuning**:  The discussion highlights the challenges of hyperparameter tuning and the potential for experimenting with optimizers to create \\\"hyperparameter hell.\\\" ([1], score: 1000, replies: 3)\\n*   **Alternative and Gradient-less Algorithms**: The conversation touches on more recent advancements in optimization algorithms, including Shampoo, SOAP, and heavyball, as well as gradient-less algorithms like Nelder-Mead. ([1.2], score: 923, replies: 1)\\n*   **Relevance to Interviews and Practical Use**: The discussion delves into the role of optimization algorithms in machine learning interviews. ([4.1], score: 269, replies: 1) Participants note that while theoretical knowledge is valuable, practical considerations like framework support and ease of use often drive algorithm choices in production.\\n\\n# Relevance of the Article\\nThis theme discusses the age of the article and the implications for the current state of optimization algorithms.\\n*   The article is from 2016. ([1], score: 1000)\\n*   Users state that the algorithm discussed in the article are rarely used now. ([4.1], score: 269)\\n\\n# AdamW's Dominance\\nThis theme highlights AdamW's prominence as a widely-used optimizer.\\n*   AdamW is used, and is considered a \\\"safe default\\\". ([1], score: 1000)\\n\\n    *   \\\"These days I rarely see much besides AdamW in production.Messing with optimizers is one of the ways to enter hyperparameter hell: it\\u2019s like legacy code but on steroids because changing it only breaks your training code stochastically. Much better to stop worrying and love AdamW.\\\" ([1], score: 1000)\\n*   Teams tend to stick with AdamW due to its ease of use and wide support. ([1.3.1], score: 769)\\n\\n# Optimizer Choice and Hyperparameter Tuning\\nThis theme touches on the complexities of selecting and tuning optimization algorithms.\\n*   Messing with optimizers can lead to \\\"hyperparameter hell\\\". ([1], score: 1000)\\n*   Demon Adam did not become standard, for similar reasons. ([1.3.1], score: 769)\\n\\n    *   \\\"Demon Adam didn\\u2019t become standard largely for the same reason many \\u201cbetter\\u201d optimizers never see wide adoption: it\\u2019s a newer tweak, not clearly superior on every problem, is less familiar to most engineers, and isn\\u2019t always bundled in major frameworks.\\\" ([1.3.1], score: 769)\\n*   The discussion touches on how complex the optimization algorithm can get. ([2.2.1], score: 423)\\n\\n    *   \\\"In n dimensions, the first derivative is an n-element vector. The second derivative is an n x n (symmetric) matrix. As n grows, the computation required to estimate the matrix increases (as at least n^2) and computation needed to use it increases (possibly faster).\\\" ([2.2.1], score: 423)\\n\\n# Alternative and Gradient-less Algorithms\\nThis theme covers alternative options for optimization.\\n*   Other optimizers are mentioned. ([1.2], score: 923)\\n    *   \\\"Luckily we have Shampoo, SOAP, Modula, Schedule-free variants, and many more these days being researched!\\\" ([1.2], score: 923)\\n*   Gradient-less algorithms are also discussed, and one user likes Nelder-Mead for non deep learning applications. ([2.1], score: 692)\\n\\n# Relevance to Interviews and Practical Use\\nThis theme looks at the information related to machine learning interviews.\\n*   The article is a good summary. ([4], score: 307)\\n*   Users state that the algorithms mentioned may be needed in interviews. ([4.1.1], score: 230)\\n\\n<|eot_id|><|eot_id|>\"\n",
            "}\n",
            "\n",
            "Length of 'text' property in the datasets:\n",
            "Row 1: post_id: 42803774. Text length: 11903.\tToken count: 2912\n",
            "Row 2: post_id: 42931109. Text length: 9925.\tToken count: 2344\n",
            "Row 3: post_id: 42901616. Text length: 13243.\tToken count: 3197\n",
            "Row 4: post_id: 42684257. Text length: 11421.\tToken count: 2803\n",
            "Row 5: post_id: 42889786. Text length: 16704.\tToken count: 3841\n",
            "Row 6: post_id: 42681762. Text length: 12948.\tToken count: 2899\n",
            "\n",
            "Total tokens in dataset: 17996. Longest token length in dataset: 3841. Model's max token length: 4096\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 171,
          "referenced_widgets": [
            "45005daed7c34075b119820a8701cfc3",
            "a041e86662b14ed8af5881003f0f99c6",
            "8f1eae582dc546dfb81157aed514f878",
            "4695a0ccb846446da94404876a40c770",
            "c2ceeb9c76954be487aafaea1e27bdc5",
            "5b0f1f13ce9b44dd871ca4f610e6d7e2",
            "35e4f6defd204ae193288176faf0bc55",
            "e3303ed5e23a4765902d2d9142490a04",
            "2c6f6b976c6b4df2a649d7877ee628c1",
            "acb57c408d8b4a058ce700fe2a94fdaa",
            "f7ca5c4d3cdb46b6bce096ec62a5c530"
          ]
        },
        "id": "95_Nn-89DhsL",
        "outputId": "54ae0388-d083-4edc-80df-cf041ccb8ef8"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250214_171219-1rxzixik</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/annjose-self/HN-Summarize%20FineTune%20DeepSeek-R1-Distill-Llama-8B%20using%20HN%20Comments%20Data/runs/1rxzixik' target=\"_blank\">llama_4096_6posts_below_4k_tokensize</a></strong> to <a href='https://wandb.ai/annjose-self/HN-Summarize%20FineTune%20DeepSeek-R1-Distill-Llama-8B%20using%20HN%20Comments%20Data' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/annjose-self/HN-Summarize%20FineTune%20DeepSeek-R1-Distill-Llama-8B%20using%20HN%20Comments%20Data' target=\"_blank\">https://wandb.ai/annjose-self/HN-Summarize%20FineTune%20DeepSeek-R1-Distill-Llama-8B%20using%20HN%20Comments%20Data</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/annjose-self/HN-Summarize%20FineTune%20DeepSeek-R1-Distill-Llama-8B%20using%20HN%20Comments%20Data/runs/1rxzixik' target=\"_blank\">https://wandb.ai/annjose-self/HN-Summarize%20FineTune%20DeepSeek-R1-Distill-Llama-8B%20using%20HN%20Comments%20Data/runs/1rxzixik</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=2):   0%|          | 0/6 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "45005daed7c34075b119820a8701cfc3"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title Create SFT trainer\n",
        "\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "import wandb\n",
        "\n",
        "# Initialize Weights and Biases for reporting\n",
        "\n",
        "# if there is a pending run, finish it before login again\n",
        "if wandb.run is not None:\n",
        "    wandb.finish()\n",
        "    print(\"Finished previous run.\")\n",
        "\n",
        "wandb.login()\n",
        "\n",
        "current_run_name = \"llama_4096_6posts_below_4k_tokensize\"\n",
        "run = wandb.init(\n",
        "    project = 'HN-Summarize FineTune DeepSeek-R1-Distill-Llama-8B using HN Comments Data',\n",
        "    name = current_run_name,\n",
        ")\n",
        "\n",
        "# TODO: Review all these params, change these - refer https://docs.unsloth.ai/basics/tutorial-how-to-finetune-llama-3-and-use-in-ollama\n",
        "# check if we want to increase gradient_accumulation_steps\n",
        "# replace max_steps with num_train_epochs: 1 or 3. We normally suggest 1 to 3 passes, and no more, otherwise you will over-fit your finetune.\n",
        "# learning_rate: adjust based on observing the output. Your job is to set parameters to make this go to as close to 0.5 as possible!\n",
        "\n",
        "# Create SFTTrainer with the base model, tokenizer and our formatted dataset\n",
        "trainer = SFTTrainer(\n",
        "    model = lora_model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dataset_num_proc = 2,\n",
        "    packing = False, # Can make training 5x faster for short sequences.\n",
        "    args = TrainingArguments(\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4,\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 60,  # for full training runs, comment this and uncomment num_train_epochs\n",
        "        # num_train_epochs = 1, # For longer training runs!\n",
        "        learning_rate = 2e-4,\n",
        "        fp16 = not is_bfloat16_supported(),\n",
        "        bf16 = is_bfloat16_supported(),\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to = \"wandb\", # Use this for WandB etc,\n",
        "        run_name = current_run_name,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ejIt2xSNKKp",
        "outputId": "c22d32b1-ab7f-4a60-c7a9-c771d4060034"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU = Tesla T4. Max memory = 14.741 GB.\n",
            "4.994 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "#@title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 723
        },
        "id": "yqxqAZ7KJ4oL",
        "outputId": "107b8e83-303f-458d-ddbb-4d09b0371183"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Longest token length in dataset: 3841. Model's max token length: 4096\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs = 1\n",
            "   \\\\   /|    Num examples = 6 | Num Epochs = 60\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient Accumulation steps = 4\n",
            "\\        /    Total batch size = 8 | Total steps = 60\n",
            " \"-____-\"     Number of trainable parameters = 24,313,856\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='0' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 0/60 : < :, Epoch 0/60]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td></td></tr><tr><td>train/global_step</td><td></td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>1.3589514342580224e+16</td></tr><tr><td>train/epoch</td><td>0</td></tr><tr><td>train/global_step</td><td>0</td></tr><tr><td>train_loss</td><td>138158.79822</td></tr><tr><td>train_runtime</td><td>1157.6194</td></tr><tr><td>train_samples_per_second</td><td>0.415</td></tr><tr><td>train_steps_per_second</td><td>0.052</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">llama_4096_6posts_below_4k_tokensize</strong> at: <a href='https://wandb.ai/annjose-self/HN-Summarize%20FineTune%20DeepSeek-R1-Distill-Llama-8B%20using%20HN%20Comments%20Data/runs/1rxzixik' target=\"_blank\">https://wandb.ai/annjose-self/HN-Summarize%20FineTune%20DeepSeek-R1-Distill-Llama-8B%20using%20HN%20Comments%20Data/runs/1rxzixik</a><br> View project at: <a href='https://wandb.ai/annjose-self/HN-Summarize%20FineTune%20DeepSeek-R1-Distill-Llama-8B%20using%20HN%20Comments%20Data' target=\"_blank\">https://wandb.ai/annjose-self/HN-Summarize%20FineTune%20DeepSeek-R1-Distill-Llama-8B%20using%20HN%20Comments%20Data</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250214_171219-1rxzixik/logs</code>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#@title Start the training\n",
        "print(f\"\\nLongest token length in dataset: {longest_token_length}. Model's max token length: {max_seq_length}\\n\\n\")\n",
        "\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "# finish logging in Weights and Biases\n",
        "wandb.finish()\n",
        "\n",
        "# set parameters to make the tranining loss go to as close to 0.5 as possible!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCqnaKmlO1U9",
        "outputId": "f5e07c0e-6cf7-44c0-ba3e-1d36bde2c9ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken for training: 1157.6194 seconds, 19.29 minutes.\n",
            "\n",
            "Peak reserved memory = 4.994 GB.\n",
            "Peak reserved memory for training = 0.0 GB.\n",
            "Peak reserved memory % of max memory = 33.878 %.\n",
            "Peak reserved memory for training % of max memory = 0.0 %.\n"
          ]
        }
      ],
      "source": [
        "#@title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory         /max_memory*100, 3)\n",
        "lora_percentage = round(used_memory_for_lora/max_memory*100, 3)\n",
        "\n",
        "print(f\"Time taken for training: {trainer_stats.metrics['train_runtime']} seconds, {round(trainer_stats.metrics['train_runtime']/60, 2)} minutes.\\n\")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! Unsloth makes inference natively 2x faster as well! You should use prompts which are similar to the ones you had finetuned on, otherwise you might get bad results!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR3gIAX-SM2q",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c5e74de798a840c5ac580538525e4221",
            "c2714e07a9a148be84bed6695c3ca6bd",
            "4f0511c1c5094046b125932400a2e2de",
            "6c12e4478fb24a27ab53d42f1bf47e12",
            "00ef845cdc9a46c89ca74ab3b1d82d57",
            "05eda4cdb21f428a98d25b41b60c52c7",
            "71d4e4f05cc54147aabfe290133c2354",
            "e841ba7baebb4325a96d0bc9df82afa9",
            "83b9b28907c84a588596cf1a44e93790",
            "a93691fda41d424b8b3f9a74e77f42fb",
            "67b6494218724968bab41c437922613d"
          ]
        },
        "outputId": "9346d502-9a71-4a44-c488-ceb260209bed"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Filter:   0%|          | 0/2 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c5e74de798a840c5ac580538525e4221"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test data found with post_id: 42864221\n",
            "Tokenized input text ... (should be in this format: |begin_of_text|><|start_header_id|>system<|end_header_id|>..system..prompt...<|start_header_id|>user<|end_header_id|>...user_prompt...\n",
            "['<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 14 Feb 2025\\n\\nYou are an AI assistant specialized in analyzing and summarizing Hacker News discussions.\\nA discussion consists of threaded comments where each comment can have child comments (replies) nested underneath it,\\nforming interconnected conversation branches. Your task is to provide concise, meaningful summaries that capture the\\nessence of the discussion while prioritizing engaging and high quality content.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nThis is your input:\\n The title of the post and comments are separated by dashed lines.\\n---- Post Title: \\nThe doctor who gave himself an ulcer and solved a medical mystery (2010)\\n----- Comments: \\n[1] (score: 1000) <replies: 1> thomasfedb: Studied at the home University of this Nobel prize winner. Was a good bet youd get a question on it in the exam every year.\\n[1.1] (score: 975) <replies: 0> dbetteridge: UWA?I was gonna say I remember always seeing posters around the Uni talking about this guy and his \\'gutsy\\' bet.\\n[2] (score: 951) <replies: 1> fiftyacorn: It\\'s great practical science but the basis of the research was a photo showing the bacteria in the stomach lining, which was against the accepted belief at the time. So the experiment confirmed what they already knew, at least that\\'s how I remember them explaining it in a documentary\\n[2.1] (score: 926) <replies: 1> adrian_b: The experiment proved that it was causation, not correlation.\\n[2.1.1] (score: 902) <replies: 0> fiftyacorn: but they could have equally proved it by giving antibiotics to people with ulcersit would be interesting if there was a body of evidence of people being cured of ulcers following antibiotics around the same time\\n[3] (score: 878) <replies: 3> camtarn: The mystery being \\'what causes ulcers\\', and the answer being Helicobacter pylori instead of stress.\\n[3.1] (score: 853) <replies: 1> Retric: He discovered an effective treatment for many ulcers, but the older one also tended to work.Stress harms the immune system so many people who dramatically reduced stress showed meaningful improvement.  One of those its better than a placebo treatments where the method of action was poorly understood.\\n[3.1.1] (score: 829) <replies: 1> manmal: H Pylori builds biofilm, and can even hide in fungal vacuoles to avoid immune system action: Its one of those cases where some bodies might just not be able to finish it off even in ideal conditions.\\n[3.1.1.1] (score: 804) <replies: 1> Retric: It was less effective, but treatments that work on say 1/2 the population arent useless.Its a tricky thing in medicine.  In then 90s people discovered Leptin a hormone released by fat which when given to people dieting significantly reduced cravings and increased energy expenditure enabling long term weight loss.  rodents and humans that become obese on a high-fat (Western) diet do not respond to leptin Unlike GLP-1 antagonists it did almost nothing to start the process of weight loss, but having a drug to give to people who lost 20+lb which would then help them keep it off could have helped a lot of people.\\n[3.1.1.1.1] (score: 780) <replies: 1> manmal: I remember cheat days in low carb diets were touted as leptin reset days.\\n[3.1.1.1.1.1] (score: 756) <replies: 0> Retric: Sadly it was BS the underlying mechanisms arent so easy to fool.  However the information was being transmitted from individual fat cells to the rest of the body through chemical pathways we can fool via drugs.\\n[3.2] (score: 731) <replies: 0> thomasfedb: The previous dogma being that no bacteria could survive in the acidic conditions (pH ~2) of the stomach.\\n[3.3] (score: 707) <replies: 0> throw83489448: Nice, but most people have some helicobacter without much problems. Some ulcers are not even caused by helicobacter.\\n[4] (score: 682) <replies: 0> dang: Related. Others?Robin Warren, pathologist who rewrote the science on ulcers, has died -  - Aug 2024 (100 comments)\\n[5] (score: 658) <replies: 8> dang: [stub for offtopicness]\\n[5.1] (score: 634) <replies: 4> whycome: The slight pause as your brain parses this headline really shows why it makes no sense to have capitalization like this in headlines. It just adds ambiguity.\\n[5.1.1] (score: 609) <replies: 1> poulpy123: It\\'s not the capitalisation it\\'s the clickbait\\n[5.1.1.1] (score: 585) <replies: 0> hnlmorg: It would definitely be the capitalisation for people who are familiar with British sci-fi.\\n[5.1.2] (score: 560) <replies: 0> pona-a: My mind went to some horrible production accident. The first episode aired only 24 yeas after asbestos snow, after all.\\n[5.1.3] (score: 536) <replies: 0> rplnt: And there are much worse examples too. But maybe that\\'s the idea, write a horrible title that makes readers pause.\\n[5.1.4] (score: 512) <replies: 1> sandworm101: Yup.  I read the title couple times before realizing it probably wasnt about Darleks.\\n[5.1.4.1] (score: 487) <replies: 1> lcnPylGDnU4H9OF: > DarleksSomething interesting about the actors accents is it always sounds like it would be spelled this way. Its actually Dalek, pronounced in a way that is nearly indistinguishable from a British person saying Darlek. (I also notice the auto-correct on my phone knows the difference, which I did not think about until I saw it.)\\n[5.1.4.1.1] (score: 463) <replies: 1> cf100clunk: That actor undoubtedly calls Canada \\'\\'Can-a-der\\'\\', as some Brits unfortunately do. Likewise with the Nigerian-British singer Sade (pronounced Shah-day) being called \"Shar-day\".> Nigerian-British singer Sade\\n[5.1.4.1.1.1] (score: 439) <replies: 0> ahartmetz: And I here was sure that the name was French, like the dude who had sadism named after him.\\n[5.2] (score: 414) <replies: 1> tolerance: While were riffing on the headline:- I must be one of the few people who did not find it ambiguous.- I am likely among fewer who dont mind it being clickbaity. I probably wouldnt have cared to know what an Australian did to learn about ulcers otherwise.But good to know if the time arises to chronicle the absurdities of modern science.\\n[5.2.1] (score: 390) <replies: 0> sethev: Yeah, I re-read the headline a couple times after seeing the comments and can\\'t spot the ambiguity. Also, there\\'s a difference between \\'attention grabbing\\' and \\'clickbait\\'. This headline seems like the first. It\\'s literally highlighting one of the most interesting parts of the story and precisely describing it.(Yes, I know the doctor in the article already believed he knew the answer)\\n[5.3] (score: 365) <replies: 1> jiehong: Had a fun time parsing the title as unrelated to The Doctor at firstI am grateful for doctors like them, and for the results they bring, even if the methods can be discussed.\\n[5.3.1] (score: 341) <replies: 0> ben30: Yeah I thought it was about a Matt Smith, 11th doctor recap, the word broth sounds vaguely mystical.\\n[5.4] (score: 317) <replies: 0> xeonmc: Anyone else thought from the title alone that this was gonna be about Louis Pasteur and the swan neck flask experiment?\\n[5.5] (score: 292) <replies: 2> chgs: Did not include David TennantWhy are all those words capitalised?\\n[5.5.1] (score: 268) <replies: 4> latexr: > Why are all those words capitalised?\\n[5.5.1.1] (score: 243) <replies: 1> skrebbel: I am unreasonably sad that the wikipedia editors couldn\\'t find it in themselves to let this one article break wikipedia\\'s URL capitalization rules.\\n[5.5.1.1.1] (score: 219) <replies: 0> 1f60c: That reminds me of the debate surrounding Star Trek Into/into Darkness \\n[5.5.1.2] (score: 195) <replies: 0> ghaff: It\\'s fairly common in presentations for the heading an a page, maybe in particular. But I do tend to avoid it personally.\\n[5.5.1.3] (score: 170) <replies: 0> shellac: Not noted there, but I think use in headlines is restricted to the US (?). This does illustrate the problem with over use.\\n[5.5.1.4] (score: 146) <replies: 0> Diti: \\n[5.5.2] (score: 121) <replies: 0> CoastalCoder: Other acceptable answers are \"Christopher Eccleston\" and \"Tom Baker\".\\n[5.6] (score: 97) <replies: 0> cwillu: mutters something about a blue box\\n[5.7] (score: 73) <replies: 0> imchillyb: There have been many doctors.But The Doctor Who, theres only one of those.  Oh.  Wait.Headlines like this should not be capitalized.  The ambiguity is unnecessary.\\n[5.8] (score: 48) <replies: 1> pipes: Click bait headline.\\n[5.8.1] (score: 24) <replies: 0> camtarn: OP here: I agree, actually - unfortunately due to HN title length limitations, I had to cut something out, and I chose to cut out the \"gave himself an ulcer\" bit. I tried to mitigate it by posting a TL;DR as the first comment :)<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n']\n",
            "\n",
            "\n",
            "...Inference: Generating summary using the fine-tuned model....\n",
            "\n",
            " ===== Output from the fine-tuned model ====\n",
            "# Overview\n",
            "The post discusses a 2010 documentary about the medical mystery of ulcers and how it was solved by a doctor who gave himself an ulcer. The comment section explores the details of the discovery, the doctor's identity, and the treatment that followed. The discussion delves into the science of medicine, the importance of empirical research, and the complexities of human health.\n",
            "\n",
            "# Main Themes & Key Insights\n",
            "*   The documentary explored the medical mystery of ulcers and how it was solved by a doctor who gave himself an ulcer.\n",
            "*   The discovery was made by Robin Warren, a pathologist, who identified the Helicobacter pylori bacteria as the cause of ulcers.\n",
            "*   The bacteria were found to be building biofilms and hiding in fungal vacuoles to avoid the immune system.\n",
            "*   The discussion highlights the importance of empirical research and experimentation in medicine.\n",
            "*   The case study illustrates the value of iterative experimentation.\n",
            "\n",
            "# Relevant Quotes & Select Comments\n",
            "*   \"[The mystery being 'what causes ulcers', and the answer being Helicobacter pylori instead of stress.]\" A medical mystery is solved.\n",
            "*   \"[He discovered an effective treatment for many ulcers, but the older one also tended to work.]\" \"Stress harms the immune system so many people who dramatically reduced stress showed meaningful improvement.  One of those its better than a placebo treatments where the method of action was poorly understood.\" - Retric\n",
            "*   \"[It was less effective, but treatments that work on say 1/2 the population dont useless]Its a tricky thing in medicine.  In then 90s people discovered Leptin a hormone released by fat which when given to people dieting significantly reduced cravings and energy expenditure enabling long term weight loss.  rodents and humans that become obese on a a high-fat (Western) diet do not respond to leptin Unlike GLP-1 antagonists it did almost nothing to start the process of weight loss, but having a drug to give to people who lost 20+lb which would then help them keep it off could have helped a lot of people.\" - Retric\n",
            "*   \"[The slight pause as your brain parses this headline really shows why it makes no sense to have capitalization like this in headlines. It just adds ambiguity.]\" This comment highlights the importance of clear communication in headlines and the potential for ambiguity through capitalization.\n",
            "*   \"[I remember cheat days in low carb diets were touted as leptin reset days.]\" A user compares the behavior of leptin with an alternative, which is used.\n",
            "*   \"[The previous dogma being that no bacteria could survive in the acidic conditions (pH ~2) of the stomach.]\" A biological fact is shared.\n",
            "*   \"[The slight pause as your brain parses this headline really shows why it makes no sense to have capitalization like this in headlines. It just adds ambiguity.]\" Users point out the potential for a headline to be misleading by using all capital letters.\n",
            "*   \"[It would definitely be the capitalisation for people who are familiar with British sci-fi.]\" A user explains the specific reference they are making.\n",
            "*   \"[The mistake is the slight pause needed to read the headline in the human brain, which is very easy to do since most people these days read with their faces]\" A user explains the potential for a \"sleigh bat\" joke and how it relates to a different type of \"sleigh\" (a vehicle).\n",
            "*   \"[It would definitely be the capitalisation] Of course it would. People who are familiar with British sci-fi will know this. It's a reference to the Doctor being British and using \"Dalek\" (which is pronounced \"Dallik\") instead of \"Lelek\". Many non-Brits will use \"Dalek\" and just call the thing he's in, and some will even call \"The Man Down The Asylum Panel\" for that matter.\" - latexr\n",
            "*   \"[I am grateful for doctors like them, and for the results they bring, even if the methods can be discussed.]\" A user expresses appreciation for the doctor's discovery.\n",
            "*   \"[The slight pause as your brain parses this headline really shows why it makes no sense to have capitalization like this in headlines. It just adds ambiguity.]\" Users state that the ambiguity is unnecessary.\n",
            "*   \"[I would also like to see some of the treatments that are used to treat other health issues, especially those related to the digestive system.]\" A user expands the conversation to inquire about other treatments.\n",
            "*   \"[The discussion highlights the importance of empirical research and experimentation in medicine.]\" This comment summarizes the main theme of the discussion.\n",
            "*   \"[a) An experiment may be used to confirm a hypothesis. b) A hypothesis may be formed from an experiment.]\" A user explains the difference between hypothesis and evidence.\n",
            "*   \"[Its a tricky thing in medicine.  In then 90s people discovered Leptin a hormone released\n",
            "\n",
            "\n",
            "...Inference: Generating summary using the base model unsloth/Llama-3.2-3B-Instruct-bnb-4bit ....\n",
            "\n",
            " ===== Output from the Base model ====\n",
            "# Overview\n",
            "The post discusses a 2010 documentary about the medical mystery of ulcers and how it was solved by a particular doctor who gave himself the problem. The comments explore the specifics of the discovery, the doctor's method, and alternative explanations. The discussion veers into a discussion of names in English and spoken language, as well as the difference between attention-getting headlines and what are considered to be genuine information exchanges on the internet.\n",
            "\n",
            "# Main Themes & Key Insights\n",
            "*   The documentary explored the role of Helicobacter pylori bacteria in causing ulcers.\n",
            "*   The discovery was made by a doctor who gave himself the problem and used a unique treatment method.\n",
            "*   The conversation touches on how medicine can work and the potential for treatments to be discovered by trial and error.\n",
            "*   The conversation also touches on how language and spelling are used in the English speaking world, with several users explaining the differences between common spellings.\n",
            "\n",
            "# Relevant Full Comments\n",
            "---\n",
            "\n",
            "[1] (score: 1000) <replies: 1> thomasfedb: Studied at the home University of this Nobel prize winner. Was a good bet youd get a question on it in the exam every year.[1.1] (score: 975) <replies: 0> dbetteridge: UWA?I was gonna say I remember always seeing posters around the Uni talking about this guy and his 'gutsy' bet.[2] (score: 951) <replies: 1> fiftyacorn: It's great practical science but the basis of the research was a photo showing the bacteria in the stomach lining, which was against the accepted belief at the time. So the experiment confirmed what they already knew, at least that's how I remember them explaining it in a documentary[2.1] (score: 926) <replies: 1> adrian_b: The experiment proved that it was causation, not correlation.[2.1.1] (score: 902) <replies: 0> fiftyacorn: but they could have equally proved it by giving antibiotics to people with ulcersit would be interesting if there was a body of evidence of people being cured of ulcers following antibiotics around the same time[3] (score: 878) <replies: 3> camtarn: The mystery being 'what causes ulcers', and the answer being Helicobacter pylori instead of stress.[3.1] (score: 853) <replies: 1> Retric: He discovered an effective treatment for many ulcers, but the older one also tended to work.Stress harms the immune system so many people who dramatically reduced stress showed meaningful improvement.  One of those its better than a placebo treatments where the method of action was poorly understood.[3.1.1] (score: 829) <replies: 1> manmal: H Pylori builds biofilm, and can even hide in fungal vacules to avoid immune system action: Its one of those cases where some bodies might just not be able to finish it off even in ideal conditions.[3.1.1.1] (score: 804) <replies: 1> Retric: It was less effective, but treatments that work on say 1/2 the population arent useless.Its a tricky thing in medicine.  In then 90s people discovered Leptin a hormone released by fat which when given to people dieting significantly reduced cravings and energy expenditure enabling long term weight loss.  rodents and humans that become obese on a high-fat (Western) diet do not respond to leptin Unlike GLP-1 antagonists it did almost nothing to start the process of weight loss, but having a drug to give to people who lost 20+lb which would then help them keep it off could have helped a lot of people.[3.2] (score: 731) <replies: 0> thomasfedb: The previous dogma being that no bacteria could survive in the acidic conditions (pH ~2) of the stomach.[3.3] (score: 707) <replies: 0> throw83489448: Nice, but most people have some helicobacter without much problems. Some ulcers are not even caused by helicobacter and the entire conversation seems to be misirected by one very interesting (and very rare) person being double named.[5] (score: 682) <replies: 8> dang: [stub for offtopicness] [5.1] (score: 634) <replies: 4> whycome: The slight pause as your brain parses this headline really shows why it makes no sense to have capitalization like this in headlines. It just adds ambiguity.[5.1.1] (score: 609) <replies: 1\n"
          ]
        }
      ],
      "source": [
        "# Load validation data from HF and run inference on the fine-tuned model\n",
        "import os\n",
        "\n",
        "# test_post_id = \"42608436\"\n",
        "test_post_id = \"42864221\"  # from the small dataset\n",
        "\n",
        "# Load test data from your HuggingFace validation dataset\n",
        "dataset = load_dataset(hf_dataset_id, split=\"validation\", token=hf_token)\n",
        "\n",
        "test_data_row = dataset.filter(lambda x: x['post_id'] == test_post_id)\n",
        "if len(test_data_row) == 0:\n",
        "    print(f\"No test data found for post_id: {test_post_id}\")\n",
        "    raise ValueError(f\"Test data not found for post_id: {test_post_id}\")\n",
        "else:\n",
        "  print(f\"Test data found with post_id: {test_data_row[0]['post_id']}\")\n",
        "\n",
        "test_input_comment = test_data_row[0]['input_comment'] or (\"Empty Comment 0000\")\n",
        "\n",
        "user_prompt = f\"{user_prompt_prefix}\\n{test_input_comment}\"\n",
        "\n",
        "# Load the fine-tuned model for inference\n",
        "FastLanguageModel.for_inference(lora_model) # Enable native 2x faster inference\n",
        "\n",
        "messages = [\n",
        "      {\"role\": \"system\", \"content\": f\"{system_prompt}\"},\n",
        "      {\"role\": \"user\", \"content\": f\"{user_prompt}\"},\n",
        "]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "# print(\"Tokenized input text ... (should be in this format: <beginofsentence><User>...user_prompt...<Assistant>\")\n",
        "print(\"Tokenized input text ... (should be in this format: |begin_of_text|><|start_header_id|>system<|end_header_id|>..system..prompt...<|start_header_id|>user<|end_header_id|>...user_prompt...\")\n",
        "decoded_input = tokenizer.batch_decode(input_ids)\n",
        "print(decoded_input)\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "\n",
        "print(\"\\n\\n...Inference: Generating summary using the fine-tuned model....\")\n",
        "print(f\"\\n ===== Output from the fine-tuned model ====\")\n",
        "_ = lora_model.generate(input_ids, streamer = text_streamer, max_new_tokens = 1024, pad_token_id = tokenizer.eos_token_id)\n",
        "\n",
        "# Load the base model for inference\n",
        "FastLanguageModel.for_inference(base_model) # Enable native 2x faster inference\n",
        "\n",
        "print(f\"\\n\\n...Inference: Generating summary using the base model {base_model_name} ....\")\n",
        "print(f\"\\n ===== Output from the Base model ====\")\n",
        "_ = base_model.generate(input_ids, streamer = text_streamer, max_new_tokens = 1024, pad_token_id = tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upcOlWe7A1vc",
        "outputId": "6a171baf-c256-480b-ae71-2b677532200e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('hn_llama_lora_model/tokenizer_config.json',\n",
              " 'hn_llama_lora_model/special_tokens_map.json',\n",
              " 'hn_llama_lora_model/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "lora_model.save_pretrained(\"hn_llama_lora_model\") # Local saving\n",
        "tokenizer.save_pretrained(\"hn_llama_lora_model\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEEcJ4qfC7Lp"
      },
      "source": [
        "Now if you want to load the LoRA adapters we just saved for inference, set `False` to `True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKX_XKs_BNZR"
      },
      "outputs": [],
      "source": [
        "load_saved_model = False\n",
        "#if False:\n",
        "if load_saved_model:\n",
        "    lora_model_name = \"hn_llama_lora_model\" # YOUR MODEL YOU USED FOR TRAINING\n",
        "    print(\"loading saved model\")\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = lora_model_name,\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "pass\n",
        "\n",
        "filename = \"test_201_comments.md\"\n",
        "test_data = load_test_data_from_file(filename) or \"hello world\"\n",
        "# print(f\"\\nTest data:\\n{test_data}\\n\")\n",
        "\n",
        "messages = [                    # Change below!\n",
        "    {\"role\": \"user\", \"content\": \"Summarize this text. \\n Your input is: {test_data}\"},\n",
        "]\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt = True,\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(input_ids, streamer = text_streamer, max_new_tokens = 128, pad_token_id = tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQMjaNrjsU5_"
      },
      "source": [
        "You can also use Hugging Face's `AutoModelForPeftCausalLM`. Only use this if you do not have `unsloth` installed. It can be hopelessly slow, since `4bit` model downloading is not supported, and Unsloth's **inference is 2x faster**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFfaXG0WsQuE"
      },
      "outputs": [],
      "source": [
        "if False:\n",
        "    # I highly do NOT suggest - use Unsloth if possible\n",
        "    from peft import AutoPeftModelForCausalLM\n",
        "    from transformers import AutoTokenizer\n",
        "    model = AutoPeftModelForCausalLM.from_pretrained(\n",
        "        \"lora_model\", # YOUR MODEL YOU USED FOR TRAINING\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"lora_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOFzC441vCtq"
      },
      "source": [
        "<a name=\"Ollama\"></a>\n",
        "### Ollama Support\n",
        "\n",
        "[Unsloth](https://github.com/unslothai/unsloth) now allows you to automatically finetune and create a [Modelfile](https://github.com/ollama/ollama/blob/main/docs/modelfile.md), and export to [Ollama](https://ollama.com/)! This makes finetuning much easier and provides a seamless workflow from `Unsloth` to `Ollama`!\n",
        "\n",
        "Let's first install `Ollama`!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUxcyP_UfeLl",
        "outputId": "69972ce0-9caf-41fd-b19a-fa058521990b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "############################################################################################# 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "WARNING: Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ],
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCv4vXHd61i7"
      },
      "source": [
        "Next, we shall save the model to GGUF / llama.cpp\n",
        "\n",
        "We clone `llama.cpp` and we default save it to `q8_0`. We allow all methods like `q4_k_m`. Use `save_pretrained_gguf` for local saving and `push_to_hub_gguf` for uploading to HF.\n",
        "\n",
        "Some supported quant methods (full list on our [Wiki page](https://github.com/unslothai/unsloth/wiki#gguf-quantization-options)):\n",
        "* `q8_0` - Fast conversion. High resource use, but generally acceptable.\n",
        "* `q4_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q4_K.\n",
        "* `q5_k_m` - Recommended. Uses Q6_K for half of the attention.wv and feed_forward.w2 tensors, else Q5_K.\n",
        "\n",
        "We also support saving to multiple GGUF options in a list fashion! This can speed things up by 10 minutes or more if you want multiple export formats!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FqfebeAdT073"
      },
      "outputs": [],
      "source": [
        "# Save to 8bit Q8_0\n",
        "if True: model.save_pretrained_gguf(\"model\", tokenizer,)\n",
        "# Remember to go to https://huggingface.co/settings/tokens for a token!\n",
        "# And change hf to your username!\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, token = \"\")\n",
        "\n",
        "# Save to 16bit GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"f16\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"f16\", token = \"\")\n",
        "\n",
        "# Save to q4_k_m GGUF\n",
        "if False: model.save_pretrained_gguf(\"model\", tokenizer, quantization_method = \"q4_k_m\")\n",
        "if False: model.push_to_hub_gguf(\"hf/model\", tokenizer, quantization_method = \"q4_k_m\", token = \"\")\n",
        "\n",
        "# Save to multiple GGUF options - much faster if you want multiple!\n",
        "if False:\n",
        "    model.push_to_hub_gguf(\n",
        "        \"hf/model\", # Change hf to your username!\n",
        "        tokenizer,\n",
        "        quantization_method = [\"q4_k_m\", \"q8_0\", \"q5_k_m\",],\n",
        "        token = \"\", # Get a token at https://huggingface.co/settings/tokens\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7lk6l0CuPXS"
      },
      "source": [
        "We use `subprocess` to start `Ollama` up in a non blocking fashion! In your own desktop, you can simply open up a new `terminal` and type `ollama serve`, but in Colab, we have to use this hack!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcP9omF_tN7Q"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "subprocess.Popen([\"ollama\", \"serve\"])\n",
        "import time\n",
        "time.sleep(3) # Wait for a few seconds for Ollama to load!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md3PExRLRhOc"
      },
      "source": [
        "`Ollama` needs a `Modelfile`, which specifies the model's prompt format. Let's print Unsloth's auto generated one:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h82vfNigRhiz",
        "outputId": "bcd91437-d4cf-47de-8905-475e3fc4deec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FROM {__FILE_LOCATION__}\n",
            "\n",
            "TEMPLATE \"\"\"Below are some instructions that describe some tasks. Write responses that appropriately complete each request.{{ if .Prompt }}\n",
            "\n",
            "### Instruction:\n",
            "{{ .Prompt }}{{ end }}\n",
            "\n",
            "### Response:\n",
            "{{ .Response }}<|end_of_text|>\"\"\"\n",
            "\n",
            "PARAMETER stop \"<|eot_id|>\"\n",
            "PARAMETER stop \"<|start_header_id|>\"\n",
            "PARAMETER stop \"<|end_header_id|>\"\n",
            "PARAMETER stop \"<|end_of_text|>\"\n",
            "PARAMETER stop \"<|reserved_special_token_\"\n",
            "PARAMETER temperature 1.5\n",
            "PARAMETER min_p 0.1\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer._ollama_modelfile)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6cipBJBudxv"
      },
      "source": [
        "We now will create an `Ollama` model called `unsloth_model` using the `Modelfile` which we auto generated!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDTUJv_QiaVh",
        "outputId": "66fcae42-3792-4b52-eb42-d867d9f83d69"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25ltransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data  \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gtransferring model data \n",
            "creating new layer sha256:94728011329d3d304c40e235f81f1b75580e163036c07d98382dc5548d555a34 \n",
            "creating new layer sha256:95b5361453780fb5797ce5abfe9a330f5d33fdec13d2232ef1443ee0c3a86ecc \n",
            "creating new layer sha256:57675488fe3dd2a75da06ae97984c4ce6f382208e9d989c584b22ee395bab0d8 \n",
            "creating new layer sha256:e706dd26476841ded603017f70f5b99b5be356caa859878787bfc3898d547f08 \n",
            "writing manifest \n",
            "success \u001b[?25h\n"
          ]
        }
      ],
      "source": [
        "!ollama create unsloth_model -f ./model/Modelfile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KSoKTKQukba"
      },
      "source": [
        "And now we can do inference on it via `Ollama`!\n",
        "\n",
        "You can also upload to `Ollama` and try the `Ollama` Desktop app by heading to https://www.ollama.com/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkp0uMrNpYaW",
        "outputId": "38bb3bd7-4a29-4c81-e319-388dcd96a449"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"model\":\"unsloth_model\",\"created_at\":\"2024-10-01T06:47:04.241326628Z\",\"message\":{\"role\":\"assistant\",\"content\":\"The\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2024-10-01T06:47:04.465575479Z\",\"message\":{\"role\":\"assistant\",\"content\":\" next\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2024-10-01T06:47:04.760101468Z\",\"message\":{\"role\":\"assistant\",\"content\":\" number\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2024-10-01T06:47:05.051240606Z\",\"message\":{\"role\":\"assistant\",\"content\":\" in\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2024-10-01T06:47:05.376545126Z\",\"message\":{\"role\":\"assistant\",\"content\":\" the\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2024-10-01T06:47:05.515751946Z\",\"message\":{\"role\":\"assistant\",\"content\":\" Fibonacci\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2024-10-01T06:47:05.658721744Z\",\"message\":{\"role\":\"assistant\",\"content\":\" sequence\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2024-10-01T06:47:05.795226527Z\",\"message\":{\"role\":\"assistant\",\"content\":\" after\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2024-10-01T06:47:05.923676364Z\",\"message\":{\"role\":\"assistant\",\"content\":\" \"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2024-10-01T06:47:06.053599585Z\",\"message\":{\"role\":\"assistant\",\"content\":\"8\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2024-10-01T06:47:06.187220374Z\",\"message\":{\"role\":\"assistant\",\"content\":\" is\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2024-10-01T06:47:06.316237671Z\",\"message\":{\"role\":\"assistant\",\"content\":\" \"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2024-10-01T06:47:06.448901764Z\",\"message\":{\"role\":\"assistant\",\"content\":\"13\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2024-10-01T06:47:06.585864644Z\",\"message\":{\"role\":\"assistant\",\"content\":\" (\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2024-10-01T06:47:06.712030586Z\",\"message\":{\"role\":\"assistant\",\"content\":\"the\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2024-10-01T06:47:06.835728964Z\",\"message\":{\"role\":\"assistant\",\"content\":\" sum\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2024-10-01T06:47:06.962898827Z\",\"message\":{\"role\":\"assistant\",\"content\":\" of\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2024-10-01T06:47:07.088064406Z\",\"message\":{\"role\":\"assistant\",\"content\":\" the\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2024-10-01T06:47:07.212942126Z\",\"message\":{\"role\":\"assistant\",\"content\":\" previous\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2024-10-01T06:47:07.336569966Z\",\"message\":{\"role\":\"assistant\",\"content\":\" two\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2024-10-01T06:47:07.46094096Z\",\"message\":{\"role\":\"assistant\",\"content\":\" numbers\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2024-10-01T06:47:07.593857726Z\",\"message\":{\"role\":\"assistant\",\"content\":\").\"},\"done\":false}\n",
            "{\"model\":\"unsloth_model\",\"created_at\":\"2024-10-01T06:47:07.741203726Z\",\"message\":{\"role\":\"assistant\",\"content\":\"\"},\"done_reason\":\"stop\",\"done\":true,\"total_duration\":3741960321,\"load_duration\":48967410,\"prompt_eval_count\":47,\"prompt_eval_duration\":150430000,\"eval_count\":23,\"eval_duration\":3499634000}\n"
          ]
        }
      ],
      "source": [
        "!curl http://localhost:11434/api/chat -d '{ \\\n",
        "    \"model\": \"unsloth_model\", \\\n",
        "    \"messages\": [ \\\n",
        "        { \"role\": \"user\", \"content\": \"Continue the Fibonacci sequence: 1, 1, 2, 3, 5, 8,\" } \\\n",
        "    ] \\\n",
        "    }'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zt9CHJqO6p30"
      },
      "source": [
        "And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/u54VK8m8tk) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!\n",
        "\n",
        "Try our [Ollama CSV notebook](https://colab.research.google.com/drive/1VYkncZMfGFkeCEgN2IzbZIKEDkyQuJAS?usp=sharing) to upload CSVs for finetuning!\n",
        "\n",
        "Some other links:\n",
        "1. Zephyr DPO 2x faster [free Colab](https://colab.research.google.com/drive/15vttTpzzVXv_tJwEk-hIcQ0S9FcEWvwP?usp=sharing)\n",
        "2. Llama 7b 2x faster [free Colab](https://colab.research.google.com/drive/1lBzz5KeZJKXjvivbYvmGarix9Ao6Wxe5?usp=sharing)\n",
        "3. TinyLlama 4x faster full Alpaca 52K in 1 hour [free Colab](https://colab.research.google.com/drive/1AZghoNBQaMDgWJpi4RbffGM1h6raLUj9?usp=sharing)\n",
        "4. CodeLlama 34b 2x faster [A100 on Colab](https://colab.research.google.com/drive/1y7A0AxE3y8gdj4AVkl2aZX47Xu3P1wJT?usp=sharing)\n",
        "5. Mistral 7b [free Kaggle version](https://www.kaggle.com/code/danielhanchen/kaggle-mistral-7b-unsloth-notebook)\n",
        "6. We also did a [blog](https://huggingface.co/blog/unsloth-trl) with  HuggingFace, and we're in the TRL [docs](https://huggingface.co/docs/trl/main/en/sft_trainer#accelerate-fine-tuning-2x-using-unsloth)!\n",
        "7. `ChatML` for ShareGPT datasets, [conversational notebook](https://colab.research.google.com/drive/1Aau3lgPzeZKQ-98h69CCu1UJcvIBLmy2?usp=sharing)\n",
        "8. Text completions like novel writing [notebook](https://colab.research.google.com/drive/1ef-tab5bhkvWmBOObepl1WgJvfvSzn5Q?usp=sharing)\n",
        "9. [**NEW**] We make Phi-3 Medium / Mini **2x faster**! See our [Phi-3 Medium notebook](https://colab.research.google.com/drive/1hhdhBa1j_hsymiW9m-WzxQtgqTH_NHqi?usp=sharing)\n",
        "\n",
        "<div class=\"align-center\">\n",
        "  <a href=\"https://github.com/unslothai/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png\" width=\"115\"></a>\n",
        "  <a href=\"https://ollama.com/\"><img src=\"https://raw.githubusercontent.com/unslothai/unsloth/nightly/images/ollama.png\" height=\"44\"></a>\n",
        "  <a href=\"https://discord.gg/u54VK8m8tk\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Discord button.png\" width=\"145\"></a>\n",
        "  <a href=\"https://ko-fi.com/unsloth\"><img src=\"https://github.com/unslothai/unsloth/raw/main/images/Kofi button.png\" width=\"145\"></a></a> Join Discord if you need help +  <i>Star us on <a href=\"https://github.com/unslothai/unsloth\">Github</a> </i> \n",
        "</div>"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [
        "uMuVrWbjAzhc",
        "XOFzC441vCtq"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "45005daed7c34075b119820a8701cfc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a041e86662b14ed8af5881003f0f99c6",
              "IPY_MODEL_8f1eae582dc546dfb81157aed514f878",
              "IPY_MODEL_4695a0ccb846446da94404876a40c770"
            ],
            "layout": "IPY_MODEL_c2ceeb9c76954be487aafaea1e27bdc5"
          }
        },
        "a041e86662b14ed8af5881003f0f99c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b0f1f13ce9b44dd871ca4f610e6d7e2",
            "placeholder": "",
            "style": "IPY_MODEL_35e4f6defd204ae193288176faf0bc55",
            "value": "Map(num_proc=2):100%"
          }
        },
        "8f1eae582dc546dfb81157aed514f878": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3303ed5e23a4765902d2d9142490a04",
            "max": 6,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2c6f6b976c6b4df2a649d7877ee628c1",
            "value": 6
          }
        },
        "4695a0ccb846446da94404876a40c770": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_acb57c408d8b4a058ce700fe2a94fdaa",
            "placeholder": "",
            "style": "IPY_MODEL_f7ca5c4d3cdb46b6bce096ec62a5c530",
            "value": "6/6[00:01&lt;00:00,4.74examples/s]"
          }
        },
        "c2ceeb9c76954be487aafaea1e27bdc5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b0f1f13ce9b44dd871ca4f610e6d7e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35e4f6defd204ae193288176faf0bc55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e3303ed5e23a4765902d2d9142490a04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c6f6b976c6b4df2a649d7877ee628c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "acb57c408d8b4a058ce700fe2a94fdaa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7ca5c4d3cdb46b6bce096ec62a5c530": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c5e74de798a840c5ac580538525e4221": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c2714e07a9a148be84bed6695c3ca6bd",
              "IPY_MODEL_4f0511c1c5094046b125932400a2e2de",
              "IPY_MODEL_6c12e4478fb24a27ab53d42f1bf47e12"
            ],
            "layout": "IPY_MODEL_00ef845cdc9a46c89ca74ab3b1d82d57"
          }
        },
        "c2714e07a9a148be84bed6695c3ca6bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05eda4cdb21f428a98d25b41b60c52c7",
            "placeholder": "",
            "style": "IPY_MODEL_71d4e4f05cc54147aabfe290133c2354",
            "value": "Filter:100%"
          }
        },
        "4f0511c1c5094046b125932400a2e2de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e841ba7baebb4325a96d0bc9df82afa9",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_83b9b28907c84a588596cf1a44e93790",
            "value": 2
          }
        },
        "6c12e4478fb24a27ab53d42f1bf47e12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a93691fda41d424b8b3f9a74e77f42fb",
            "placeholder": "",
            "style": "IPY_MODEL_67b6494218724968bab41c437922613d",
            "value": "2/2[00:00&lt;00:00,117.49examples/s]"
          }
        },
        "00ef845cdc9a46c89ca74ab3b1d82d57": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05eda4cdb21f428a98d25b41b60c52c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "71d4e4f05cc54147aabfe290133c2354": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e841ba7baebb4325a96d0bc9df82afa9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83b9b28907c84a588596cf1a44e93790": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a93691fda41d424b8b3f9a74e77f42fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "67b6494218724968bab41c437922613d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}